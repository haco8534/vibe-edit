# 🎬 拡散モデルによる画像生成 — 台本

> **形式**: 対話形式（解説役＋質問役）
> **想定時間**: 約10分
> **キャラクター**:
> - **シュウ**（解説役）— 落ち着いた口調で分かりやすく説明する
> - **ミク**（質問役 / 聞き手）— 素朴な疑問を投げかけ、理解を深める

---

## Scene 01: タイトル（0:00〜0:25）

**【アニメーション: タイトル表示】**

> **ミク**: ねえシュウ、最近 AI で絵が描けるって聞いたんだけど、あれってどういう仕組みなの？

> **シュウ**: いい質問だね。今日は「拡散モデル」っていう、最近の画像生成 AI の心臓部について話そう。

**【アニメーション: 「拡散モデルによる画像生成」タイトル + サブタイトル表示】**

---

## Scene 02: 画像生成って何？（0:25〜1:15）

**【アニメーション: きれいな画像 → 「ノイズから生まれた」とキャプション】**

> **ミク**: 拡散モデル？ 何かが広がっていくイメージ？

> **シュウ**: まさにそのイメージだよ。例えるなら、こういうこと。インクを水に落とすと、どんどん広がって最後は均一になるよね？

**【アニメーション: インクの拡散イメージ — 集中した色→広がって薄くなる】**

> **シュウ**: 拡散モデルは、この「広がる」プロセスの  **逆**  をやるんだ。均一なノイズから始めて、きれいな画像を「復元」する。

> **ミク**: え、逆？ バラバラのものを元に戻すってこと？

> **シュウ**: そう。でもいきなり全部は無理だから、少しずつ戻していく。ここが拡散モデルのミソなんだ。

---

## Scene 03: 前方過程 — ノイズを加える（1:15〜2:45）

**【アニメーション: セクションタイトル「Step 1: 前方過程（Forward Process）」】**

> **シュウ**: まず「前方過程」から説明するね。これは学習の準備段階で、元の画像に少しずつノイズを加えていくプロセスだよ。

**【アニメーション: きれいな画像（カラーグリッド）→ 段階的にノイズが加わる（t=0, 250, 500, 750, 1000）】**

> **シュウ**: 元のきれいな画像を x₀ として、ステップごとにガウスノイズを少しずつ加えていく。

**【アニメーション: 数式 q(xₜ|xₜ₋₁) = N(xₜ; √(1-βₜ)xₜ₋₁, βₜI) を表示】**

> **ミク**: この数式は？

> **シュウ**: 各ステップで元の画像をちょっとだけ薄めて、その分だけノイズを足してるんだ。βₜ はノイズの量を決めるパラメータだよ。

> **ミク**: で、最後はどうなるの？

> **シュウ**: 十分なステップ — 例えば1000回 — 繰り返すと、元の画像の情報は完全に消えて、ただのランダムノイズになる。

**【アニメーション: 最終的に完全なノイズ画像になる。矢印で「元画像 → 完全なノイズ」を示す】**

> **ミク**: わざわざ壊すの？ なんで？

> **シュウ**: ここがポイント。壊す過程を記録しておくと、「戻す方法」を学べるんだ。

---

## Scene 04: 核心 — ノイズ除去を学ぶ（2:45〜4:30）

**【アニメーション: セクションタイトル「Step 2: ニューラルネットワークの学習」】**

> **シュウ**: 今度は逆方向を考えよう。ノイズだらけの画像を見せられて、「どんなノイズが加わっているか」を当てるゲームだと思ってほしい。

**【アニメーション: ノイズ画像 → ニューラルネットワーク(U-Net)のボックス → 予測ノイズ、の流れ】**

> **ミク**: ノイズを当てる？

> **シュウ**: そう。ニューラルネットワークに、ノイズまみれの画像 xₜ と、今どのステップ t にいるかを教える。ネットワークは「この画像に加わったノイズはこれだろう」と予測する。

**【アニメーション: 入力（xₜ, t）→ U-Net → 予測ノイズ ε̂ を図示】**

> **シュウ**: そして、答え合わせだ。実際に加えたノイズ ε と、予測ノイズ ε̂ の差を最小にするように学習する。

**【アニメーション: 損失関数 L = ||ε − ε̂||²、実際のノイズと予測の比較】**

> **ミク**: あ、正解が分かってるから学習できるんだ！ だって、ノイズは自分で加えたものだから。

> **シュウ**: その通り！ 自分でノイズを加えたから正解を知ってる。これが拡散モデルの学習の美しいところだね。

> **ミク**: でも、全部のステップで学習するの？ 大変じゃない？

> **シュウ**: いい質問。実は毎回ランダムにステップ t を選んで学習するんだ。t=10 のときも、t=500 のときも、t=990のときも対応できるように。

---

## Scene 05: 逆過程 — 画像の生成（4:30〜6:15）

**【アニメーション: セクションタイトル「Step 3: 逆過程（Reverse Process）— 画像を生み出す」】**

> **シュウ**: さて、学習が終わったら、いよいよ画像生成だ。完全なランダムノイズから始める。

**【アニメーション: 完全なノイズ xT を表示】**

> **ミク**: このグチャグチャから絵ができるの？

> **シュウ**: できるんだよ。学習済みのネットワークに「このノイズ画像に含まれてるノイズを予測して」と頼む。予測されたノイズを引き算すると、すこしだけきれいになる。

**【アニメーション: xT → ノイズ予測 → 引き算 → xT-1（少しだけきれいに）のステップを図示】**

> **シュウ**: これを繰り返す。1000回、999回、998回…と、少しずつノイズを取り除いていく。

**【アニメーション: 段階的なデノイジングプロセス。5段階くらいで画像がどんどんきれいに。各ステップに矢印とU-Netを表示】**

> **ミク**: おお〜! だんだん絵になってきた！ まるで霧が晴れていくみたい。

> **シュウ**: いい表現だね。最初は全体的な構図や色が決まって、後半になるにつれて細かいディテールが浮かび上がってくる。粗いところから精密なところへ、段階的に画像が「結晶化」していくイメージだ。

> **ミク**: ちょっと待って。でも同じノイズから始めたら毎回同じ絵にならない？

> **シュウ**: スタートのノイズが毎回ランダムだから、出来上がる画像も毎回違うんだ。同じネットワークでも、無限のバリエーションが生まれる。

---

## Scene 06: テキスト条件付け（6:15〜7:45）

**【アニメーション: セクションタイトル「Step 4: テキストで画像を操る」】**

> **ミク**: でもさ、「猫の絵を描いて」って指定できるじゃん？ ノイズからだとランダムな絵しかできないんじゃ？

> **シュウ**: 鋭いね。ここで「テキスト条件付け」が登場する。プロンプト — 例えば「夕焼けの中を歩く猫」— を、まず数値ベクトルに変換する。

**【アニメーション: テキスト「夕焼けの中を歩く猫」→ テキストエンコーダ → ベクトル】**

> **シュウ**: この情報をノイズ除去のたびにネットワークに一緒に渡すんだ。すると、ネットワークは「猫を残す方向」にノイズを除去するようになる。

**【アニメーション: U-Netに「ノイズ画像」と「テキスト情報」の両方が入力される図。出力が猫の画像になっていく】**

> **ミク**: テキストがガイドになるんだ！ カーナビみたいに、「こっちに進め」って方向を教えてくれる感じ？

> **シュウ**: まさにそう。もう少し正確に言うと、「テキストあり」と「テキストなし」の両方でノイズを予測して、テキストありの方向を強調するんだ。これをクラシファイアフリーガイダンスっていう。

**【アニメーション: 2つの予測（条件あり/なし）の差分を矢印で表し、条件ありの方向に強調される様子】**

> **ミク**: テキストの影響度を強くしたり弱くしたりできるの？

> **シュウ**: できるよ。ガイダンススケールっていうパラメータで調整する。大きくするとテキストに忠実になるけど、画像の多様性は減る。小さくすると自由度は高いけど、指示通りにならないこともある。

---

## Scene 07: 潜在拡散モデル（7:45〜9:15）

**【アニメーション: セクションタイトル「Step 5: 潜在空間での高速化 — Stable Diffusion」】**

> **ミク**: ところでさ、512×512 の画像を1000ステップも処理するって、めちゃくちゃ時間かかりそう。

> **シュウ**: その通り。実は初期の拡散モデルは非常に遅かった。そこで登場したのが「潜在拡散モデル」、いわゆる Stable Diffusion のアーキテクチャだ。

**【アニメーション: 画像空間（大）vs 潜在空間（小）の比較図】**

> **シュウ**: アイデアはシンプル。まず画像を「オートエンコーダ」で圧縮して、小さな潜在空間に変換する。例えば 512×512 の画像を 64×64 の潜在表現に圧縮する。

**【アニメーション: エンコーダで画像が圧縮される → 潜在空間で拡散過程 → デコーダで復元、のパイプライン】**

> **ミク**: 8分の1！ それは速くなりそう。

> **シュウ**: 拡散過程をこの小さな潜在空間でやるから、計算量が劇的に減る。そしてノイズ除去が終わったら、デコーダで元のサイズの画像に復元する。

> **ミク**: 画質は落ちないの？

> **シュウ**: オートエンコーダの学習がうまくいっていれば、ほぼ劣化なく復元できる。これが Stable Diffusion の名前の由来でもあるんだ。効率的で安定（Stable）した拡散モデルってことだね。

---

## Scene 08: まとめ（9:15〜10:00）

**【アニメーション: セクションタイトル「まとめ」】**

> **シュウ**: じゃあ最後に全体の流れをおさらいしよう。

**【アニメーション: 全体のパイプラインを一つの図で表示。番号付きで順番に表示】**

> **シュウ**: 
> 1. **前方過程**: 画像にノイズを加えて壊す（学習データの準備）
> 2. **学習**: ネットワークがノイズを予測する訓練をする
> 3. **逆過程**: ランダムノイズから少しずつノイズを除去して画像を生成
> 4. **テキスト条件付け**: プロンプトでノイズ除去の方向をガイド
> 5. **潜在空間**: 圧縮された空間で拡散することで高速化

> **ミク**: 壊すことで作り方を学ぶって、なんだか禅みたいだね。

> **シュウ**: いい例えだね。壊す過程を知っているからこそ、戻す方法を学べる。これが拡散モデルの本質だよ。

> **ミク**: ありがとう！ これでイメージ掴めた気がする。

**【アニメーション: 完全なノイズ → きれいな画像への変化を最後にもう一度見せて終了】**

---

## シーン対応表

| シーン番号 | タイトル | 時間 | 秒数 |
|-----------|---------|------|------|
| Scene 01 | タイトル | 0:00〜0:25 | 25s |
| Scene 02 | 画像生成って何？ | 0:25〜1:15 | 50s |
| Scene 03 | 前方過程 | 1:15〜2:45 | 90s |
| Scene 04 | ニューラルネットワークの学習 | 2:45〜4:30 | 105s |
| Scene 05 | 逆過程 — 画像の生成 | 4:30〜6:15 | 105s |
| Scene 06 | テキスト条件付け | 6:15〜7:45 | 90s |
| Scene 07 | 潜在拡散モデル | 7:45〜9:15 | 90s |
| Scene 08 | まとめ | 9:15〜10:00 | 45s |
| **合計** | | | **600s (10:00)** |

## キャラクターデザインノート

- **シュウ**: 色は `#3b82f6`（青）。テキスト表示時は「シュウ:」の部分を青で表示。
- **ミク**: 色は `#e94560`（赤）。テキスト表示時は「ミク:」の部分を赤で表示。
- 対話テキストは画面下部に字幕風に表示し、上部でアニメーションが動く構成。
